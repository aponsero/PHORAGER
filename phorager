#!/usr/bin/env python3

import os
import re
import sys
import shutil
import argparse
import logging
import subprocess
from datetime import datetime
from pathlib import Path
import multiprocessing

## General classes
class TerminalColors:
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    RESET = '\033[0m'

class NextflowConfigParser:
    @staticmethod
    def _resolve_project_dir():
        """
        Resolve the project directory based on the current script location
        """
        return os.path.dirname(os.path.abspath(sys.argv[0]))

    @staticmethod
    def parse_config(config_path='nextflow.config'):
        """
        Parse Nextflow configuration file to extract specific parameters
        """
        # Resolve project directory
        project_dir = NextflowConfigParser._resolve_project_dir()

        try:
            with open(config_path, 'r') as f:
                config_content = f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Nextflow config file not found: {config_path}")

        # Replace $projectDir with the actual project directory path
        config_content = config_content.replace('$projectDir', project_dir)

        # Existing parsing methods...

        return {
            'db_location': NextflowConfigParser.parse_db_location(config_content),
            'conda_cache_dir': NextflowConfigParser.parse_conda_cache(config_content),
            'genome': NextflowConfigParser.parse_genome(config_content),
            'threads': NextflowConfigParser._parse_threads(config_content),
            'completeness_threshold': NextflowConfigParser.parse_completeness_threshold(config_content),
            'contamination_threshold': NextflowConfigParser.parse_contamination_threshold(config_content),
            'drep_ani_threshold': NextflowConfigParser.parse_drep_ani_threshold(config_content),
            'outdir': NextflowConfigParser._parse_outdir(config_content),
            'use_dereplicated_genomes': NextflowConfigParser._parse_use_dereplicated_genomes(config_content),
            'run_genomad': NextflowConfigParser._parse_run_genomad(config_content),
            'run_vibrant': NextflowConfigParser._parse_run_vibrant(config_content),
            'genomad_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'genomad'),
            'vibrant_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'vibrant'),
            'checkv_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'checkv'),
            'pharokka_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'pharokka'),
            'phold_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'phold'),
            'min_prophage_length': NextflowConfigParser._parse_min_prophage_length(config_content),
            'checkv_quality_levels': NextflowConfigParser._parse_checkv_quality_levels(config_content),
            'skip_detailed_annotation': NextflowConfigParser._parse_skip_detailed_annotation(config_content),
            'annotation_filter_mode': NextflowConfigParser._parse_annotation_filter_mode(config_content),
            'pharokka_structural_perc': NextflowConfigParser._parse_structural_params(config_content, 'pharokka')[0],
            'pharokka_structural_total': NextflowConfigParser._parse_structural_params(config_content, 'pharokka')[1],
            'phold_structural_perc': NextflowConfigParser._parse_structural_params(config_content, 'phold')[0],
            'phold_structural_total': NextflowConfigParser._parse_structural_params(config_content, 'phold')[1],
            'clustering_min_ani': NextflowConfigParser._parse_clustering_params(config_content)[0],
            'clustering_min_coverage': NextflowConfigParser._parse_clustering_params(config_content)[1]
        }

class NextflowConfigParser:
    @staticmethod
    def _resolve_project_dir():
        """
        Resolve the project directory based on the current script location
        """
        return os.path.dirname(os.path.abspath(sys.argv[0]))

    @staticmethod
    def parse_config(config_path='nextflow.config'):
        """
        Parse Nextflow configuration file to extract specific parameters
        """
        # Resolve project directory
        project_dir = NextflowConfigParser._resolve_project_dir()

        try:
            with open(config_path, 'r') as f:
                config_content = f.read()
        except FileNotFoundError:
            raise FileNotFoundError(f"Nextflow config file not found: {config_path}")
        except Exception as e:
            raise ValueError(f"Error reading config file: {e}")

        # Replace $projectDir with the actual project directory path
        config_content = config_content.replace('$projectDir', project_dir)

        # Parse all parameters with improved error handling
        try:
            return {
                'db_location': NextflowConfigParser.parse_db_location(config_content),
                'conda_cache_dir': NextflowConfigParser.parse_conda_cache(config_content),
                'genome': NextflowConfigParser.parse_genome(config_content),
                'threads': NextflowConfigParser._parse_threads(config_content),
                'completeness_threshold': NextflowConfigParser.parse_completeness_threshold(config_content),
                'contamination_threshold': NextflowConfigParser.parse_contamination_threshold(config_content),
                'drep_ani_threshold': NextflowConfigParser.parse_drep_ani_threshold(config_content),
                'outdir': NextflowConfigParser._parse_outdir(config_content),
                'use_dereplicated_genomes': NextflowConfigParser._parse_use_dereplicated_genomes(config_content),
                'run_genomad': NextflowConfigParser._parse_run_genomad(config_content),
                'run_vibrant': NextflowConfigParser._parse_run_vibrant(config_content),
                'genomad_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'genomad'),
                'vibrant_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'vibrant'),
                'checkv_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'checkv'),
                'pharokka_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'pharokka'),
                'phold_db_location': NextflowConfigParser._parse_db_location_for_tool(config_content, 'phold'),
                'min_prophage_length': NextflowConfigParser._parse_min_prophage_length(config_content),
                'checkv_quality_levels': NextflowConfigParser._parse_checkv_quality_levels(config_content),
                'skip_detailed_annotation': NextflowConfigParser._parse_skip_detailed_annotation(config_content),
                'annotation_filter_mode': NextflowConfigParser._parse_annotation_filter_mode(config_content),
                'pharokka_structural_perc': NextflowConfigParser._parse_structural_params(config_content, 'pharokka')[0],
                'pharokka_structural_total': NextflowConfigParser._parse_structural_params(config_content, 'pharokka')[1],
                'phold_structural_perc': NextflowConfigParser._parse_structural_params(config_content, 'phold')[0],
                'phold_structural_total': NextflowConfigParser._parse_structural_params(config_content, 'phold')[1],
                'clustering_min_ani': NextflowConfigParser._parse_clustering_params(config_content)[0],
                'clustering_min_coverage': NextflowConfigParser._parse_clustering_params(config_content)[1]
            }
        except Exception as e:
            raise ValueError(f"Error parsing Nextflow config: {e}")

    @staticmethod
    def _parse_threads(config_content):
        """
        Parse default threads from config content
        Defaults to number of available CPU cores if not specified
        """
        try:
            threads_match = re.search(r'threads\s*=\s*Runtime\.runtime\.availableProcessors\(\)', config_content)
            if threads_match:
                return multiprocessing.cpu_count()
            
            # Alternative parsing if explicit number is used
            threads_num_match = re.search(r'threads\s*=\s*(\d+)', config_content)
            if threads_num_match:
                threads = int(threads_num_match.group(1))
                if threads < 1:
                    logging.warning(f"Thread count in config ({threads}) is less than 1, defaulting to 1")
                    return 1
                return threads
            
            # Default to all available cores if no specification found
            logging.info("No thread specification found in config, defaulting to available CPU cores")
            return multiprocessing.cpu_count()
        except ValueError as e:
            logging.warning(f"Error parsing thread count from config: {e}, defaulting to available CPU cores")
            return multiprocessing.cpu_count()
        except Exception as e:
            logging.warning(f"Unexpected error parsing thread count: {e}, defaulting to available CPU cores")
            return multiprocessing.cpu_count()

    @staticmethod
    def parse_db_location(config_content):
        """
        Parse global database location from config content
        """
        try:
            db_match = re.search(r'global_db_location\s*=\s*"([^"]+)"', config_content)
            if not db_match:
                raise ValueError("global_db_location parameter not found in config")
            
            db_location = db_match.group(1)
            expanded_path = os.path.expandvars(os.path.expanduser(db_location))
            
            return expanded_path
        except ValueError as e:
            # Re-raise specific ValueError with original message
            raise e
        except Exception as e:
            raise ValueError(f"Failed to parse global database location: {e}")

    @staticmethod
    def parse_conda_cache(config_content):
        """
        Parse conda cache directory from config content
        """
        try:
            cache_match = re.search(r'conda_cache_dir\s*=\s*"([^"]+)"', config_content)
            if not cache_match:
                raise ValueError("conda_cache_dir parameter not found in config")
            
            conda_cache_dir = cache_match.group(1)
            expanded_path = os.path.expandvars(os.path.expanduser(conda_cache_dir))
            
            return expanded_path
        except ValueError as e:
            # Re-raise specific ValueError with original message
            raise e
        except Exception as e:
            raise ValueError(f"Failed to parse conda cache directory: {e}")

    @staticmethod
    def parse_genome(config_content):
        """
        Parse genome location from config content
        """
        try:
            genome_match = re.search(r'genome\s*=\s*"([^"]+)"', config_content)
            if not genome_match:
                raise ValueError("genome parameter not found in config")
            
            genome = genome_match.group(1)
            expanded_path = os.path.expandvars(os.path.expanduser(genome))
            
            return expanded_path
        except ValueError as e:
            # Re-raise specific ValueError with original message
            raise e
        except Exception as e:
            raise ValueError(f"Failed to parse genome location: {e}")

    @staticmethod
    def parse_completeness_threshold(config_content):
        """
        Parse completeness threshold from config content
        """
        try:
            threshold_match = re.search(r'completeness_threshold\s*=\s*(\d+\.?\d*)', config_content)
            if not threshold_match:
                logging.info("completeness_threshold not found in config, using default value of 95.0")
                return 95.0
            
            threshold = float(threshold_match.group(1))
            
            # Validate the threshold is in a reasonable range
            if threshold < 0 or threshold > 100:
                logging.warning(f"Completeness threshold ({threshold}) is outside the range 0-100, using anyway")
            
            return threshold
        except ValueError as e:
            logging.warning(f"Error parsing completeness threshold: {e}, using default of 95.0")
            return 95.0
        except Exception as e:
            logging.warning(f"Unexpected error parsing completeness threshold: {e}, using default of 95.0")
            return 95.0

    @staticmethod
    def parse_contamination_threshold(config_content):
        """
        Parse contamination threshold from config content
        """
        try:
            threshold_match = re.search(r'contamination_threshold\s*=\s*(\d+\.?\d*)', config_content)
            if not threshold_match:
                logging.info("contamination_threshold not found in config, using default value of 5.0")
                return 5.0
            
            threshold = float(threshold_match.group(1))
            
            # Validate the threshold is in a reasonable range
            if threshold < 0 or threshold > 100:
                logging.warning(f"Contamination threshold ({threshold}) is outside the range 0-100, using anyway")
            
            return threshold
        except ValueError as e:
            logging.warning(f"Error parsing contamination threshold: {e}, using default of 5.0")
            return 5.0
        except Exception as e:
            logging.warning(f"Unexpected error parsing contamination threshold: {e}, using default of 5.0")
            return 5.0

    @staticmethod
    def parse_drep_ani_threshold(config_content):
        """
        Parse dRep ANI threshold from config content
        """
        try:
            threshold_match = re.search(r'drep_ani_threshold\s*=\s*(\d+\.?\d*)', config_content)
            if not threshold_match:
                logging.info("drep_ani_threshold not found in config, using default value of 0.999")
                return 0.999
            
            threshold = float(threshold_match.group(1))
            
            # Validate the threshold is in a reasonable range
            if threshold < 0 or threshold > 1:
                logging.warning(f"dRep ANI threshold ({threshold}) is outside the range 0-1, using anyway")
            
            return threshold
        except ValueError as e:
            logging.warning(f"Error parsing dRep ANI threshold: {e}, using default of 0.999")
            return 0.999
        except Exception as e:
            logging.warning(f"Unexpected error parsing dRep ANI threshold: {e}, using default of 0.999")
            return 0.999

    @staticmethod
    def _parse_outdir(config_content):
        """
        Parse output directory from config content
        """
        try:
            outdir_match = re.search(r'outdir\s*=\s*"([^"]+)"', config_content)
            if not outdir_match:
                logging.info("outdir not found in config, using default value of '$projectDir/results'")
                outdir = os.path.join(NextflowConfigParser._resolve_project_dir(), "results")
            else:
                outdir = outdir_match.group(1)
            
            # Expand project directory and user home
            expanded_path = os.path.expandvars(os.path.expanduser(outdir))
            
            return expanded_path
        except Exception as e:
            logging.warning(f"Error parsing output directory: {e}, using default")
            return os.path.join(NextflowConfigParser._resolve_project_dir(), "results")
    
    @staticmethod
    def _parse_use_dereplicated_genomes(config_content):
        """
        Parse use_dereplicated_genomes parameter
        """
        try:
            match = re.search(r'use_dereplicated_genomes\s*=\s*(\w+)', config_content)
            if not match:
                logging.info("use_dereplicated_genomes not found in config, using default value of false")
                return False
            
            value = match.group(1).lower()
            if value not in ['true', 'false']:
                logging.warning(f"Invalid value for use_dereplicated_genomes: {value}, expected 'true' or 'false', defaulting to false")
                return False
                
            return value == 'true'
        except Exception as e:
            logging.warning(f"Error parsing use_dereplicated_genomes: {e}, using default of false")
            return False

    @staticmethod
    def _parse_run_genomad(config_content):
        """
        Parse run_genomad parameter
        """
        try:
            match = re.search(r'run_genomad\s*=\s*(\w+)', config_content)
            if not match:
                logging.info("run_genomad not found in config, using default value of true")
                return True
            
            value = match.group(1).lower()
            if value not in ['true', 'false']:
                logging.warning(f"Invalid value for run_genomad: {value}, expected 'true' or 'false', defaulting to true")
                return True
                
            result = value == 'true'
            logging.debug(f"run_genomad set to: {result}")
            return result
        except Exception as e:
            logging.warning(f"Error parsing run_genomad: {e}, using default of true")
            return True

    @staticmethod
    def _parse_run_vibrant(config_content):
        """
        Parse run_vibrant parameter
        """
        try:
            match = re.search(r'run_vibrant\s*=\s*(\w+)', config_content)
            if not match:
                logging.info("run_vibrant not found in config, using default value of true")
                return True
            
            value = match.group(1).lower()
            if value not in ['true', 'false']:
                logging.warning(f"Invalid value for run_vibrant: {value}, expected 'true' or 'false', defaulting to true")
                return True
                
            result = value == 'true'
            logging.debug(f"run_vibrant set to: {result}")
            return result
        except Exception as e:
            logging.warning(f"Error parsing run_vibrant: {e}, using default of true")
            return True

    @staticmethod
    def _parse_db_location_for_tool(config_content, tool):
        """
        Parse database location for a specific tool
        """
        try:
            param_name = f"{tool}_db_location"
            match = re.search(rf'{tool}_db_location\s*=\s*"([^"]+)"', config_content)
            if not match:
                raise ValueError(f"{param_name} parameter not found in config")
            
            db_location = match.group(1)
            expanded_path = os.path.expandvars(os.path.expanduser(db_location))
            
            return expanded_path
        except ValueError as e:
            # Re-raise specific ValueError with original message
            raise e
        except Exception as e:
            raise ValueError(f"Failed to parse {tool} database location: {e}")

    @staticmethod
    def _parse_min_prophage_length(config_content):
        """
        Parse minimum prophage length from config content
        """
        try:
            match = re.search(r'min_prophage_length\s*=\s*(\d+)', config_content)
            if not match:
                logging.info("min_prophage_length not found in config, using default value of 5000")
                return 5000
            
            length = int(match.group(1))
            if length <= 0:
                logging.warning(f"Invalid minimum prophage length: {length}, must be positive, defaulting to 5000")
                return 5000
                
            return length
        except ValueError as e:
            logging.warning(f"Error parsing minimum prophage length: {e}, using default of 5000")
            return 5000
        except Exception as e:
            logging.warning(f"Unexpected error parsing minimum prophage length: {e}, using default of 5000")
            return 5000

    @staticmethod
    def _parse_checkv_quality_levels(config_content):
        """
        Parse CheckV quality levels from config content
        """
        try:
            match = re.search(r'checkv_quality_levels\s*=\s*\[(.*?)\]', config_content)
            if not match:
                logging.info("checkv_quality_levels not found in config, using default values")
                return ['Medium-quality', 'High-quality', 'Complete']
            
            # Split and strip each quality level
            quality_levels_str = match.group(1)
            quality_levels = [level.strip().strip("'\"") for level in quality_levels_str.split(',')]
            
            # Validate quality levels
            valid_levels = ['Low-quality', 'Medium-quality', 'High-quality', 'Complete']
            for level in quality_levels:
                if level not in valid_levels:
                    logging.warning(f"Invalid CheckV quality level: {level}, expected one of {valid_levels}")
            
            # If empty or all invalid, use defaults
            if not quality_levels or not any(level in valid_levels for level in quality_levels):
                logging.warning("No valid CheckV quality levels found, using defaults")
                return ['Medium-quality', 'High-quality', 'Complete']
                
            return quality_levels
        except Exception as e:
            logging.warning(f"Error parsing CheckV quality levels: {e}, using default values")
            return ['Medium-quality', 'High-quality', 'Complete']

    @staticmethod
    def _parse_skip_detailed_annotation(config_content):
        """
        Parse skip detailed annotation flag from config content
        """
        try:
            match = re.search(r'skip_detailed_annotation\s*=\s*(\w+)', config_content)
            if not match:
                logging.info("skip_detailed_annotation not found in config, using default value of false")
                return False
            
            value = match.group(1).lower()
            if value not in ['true', 'false']:
                logging.warning(f"Invalid value for skip_detailed_annotation: {value}, expected 'true' or 'false', defaulting to false")
                return False
                
            return value == 'true'
        except Exception as e:
            logging.warning(f"Error parsing skip_detailed_annotation: {e}, using default of false")
            return False

    @staticmethod
    def _parse_annotation_filter_mode(config_content):
        """
        Parse annotation filter mode from config content
        """
        try:
            match = re.search(r'annotation_filter_mode\s*=\s*[\'"](\w+)[\'"]', config_content)
            if not match:
                logging.info("annotation_filter_mode not found in config, using default value of 'both'")
                return 'both'
            
            mode = match.group(1)
            valid_modes = ['both', 'pharokka', 'phold', 'none']
            if mode not in valid_modes:
                logging.warning(f"Invalid annotation filter mode: {mode}, expected one of {valid_modes}, defaulting to 'both'")
                return 'both'
                
            return mode
        except Exception as e:
            logging.warning(f"Error parsing annotation filter mode: {e}, using default of 'both'")
            return 'both'

    @staticmethod
    def _parse_structural_params(config_content, tool):
        """
        Parse structural parameters for Pharokka or PHOLD
        """
        try:
            perc_param = f"{tool}_structural_perc"
            total_param = f"{tool}_structural_total"
            
            perc_match = re.search(rf'{tool}_structural_perc\s*=\s*(\d+\.?\d*)', config_content)
            total_match = re.search(rf'{tool}_structural_total\s*=\s*(\d+)', config_content)
            
            # Default values
            default_perc = 20.0
            default_total = 3
            
            # Parse percentage
            if not perc_match:
                logging.info(f"{perc_param} not found in config, using default value of {default_perc}")
                perc = default_perc
            else:
                perc = float(perc_match.group(1))
                if perc < 0 or perc > 100:
                    logging.warning(f"Invalid {perc_param}: {perc}, must be between 0 and 100, defaulting to {default_perc}")
                    perc = default_perc
            
            # Parse total
            if not total_match:
                logging.info(f"{total_param} not found in config, using default value of {default_total}")
                total = default_total
            else:
                total = int(total_match.group(1))
                if total < 0:
                    logging.warning(f"Invalid {total_param}: {total}, must be non-negative, defaulting to {default_total}")
                    total = default_total
            
            return perc, total
        except ValueError as e:
            logging.warning(f"Error parsing {tool} structural parameters: {e}, using default values")
            return 20.0, 3
        except Exception as e:
            logging.warning(f"Unexpected error parsing {tool} structural parameters: {e}, using default values")
            return 20.0, 3

    @staticmethod
    def _parse_clustering_params(config_content):
        """
        Parse clustering parameters
        """
        try:
            default_ani = 99.0
            default_coverage = 85.0
            
            ani_match = re.search(r'clustering_min_ani\s*=\s*(\d+\.?\d*)', config_content)
            coverage_match = re.search(r'clustering_min_coverage\s*=\s*(\d+\.?\d*)', config_content)
            
            # Parse ANI
            if not ani_match:
                logging.info(f"clustering_min_ani not found in config, using default value of {default_ani}")
                ani = default_ani
            else:
                ani = float(ani_match.group(1))
                if ani < 0 or ani > 100:
                    logging.warning(f"Invalid clustering_min_ani: {ani}, must be between 0 and 100, defaulting to {default_ani}")
                    ani = default_ani
            
            # Parse coverage
            if not coverage_match:
                logging.info(f"clustering_min_coverage not found in config, using default value of {default_coverage}")
                coverage = default_coverage
            else:
                coverage = float(coverage_match.group(1))
                if coverage < 0 or coverage > 100:
                    logging.warning(f"Invalid clustering_min_coverage: {coverage}, must be between 0 and 100, defaulting to {default_coverage}")
                    coverage = default_coverage
            
            return ani, coverage
        except ValueError as e:
            logging.warning(f"Error parsing clustering parameters: {e}, using default values")
            return 99.0, 85.0
        except Exception as e:
            logging.warning(f"Unexpected error parsing clustering parameters: {e}, using default values")
            return 99.0, 85.0

## Sub-workflow classes
## Installation
class PhoragerInstaller:
    def __init__(self, 
                 db_location=None, 
                 conda_cache_dir=None, 
                 force_reinstall=False,
                 install_mode='all',
                 conda_only=False,
                 verbose=False):
        # Parse configuration
        config = NextflowConfigParser.parse_config()
        
        # Override config with CLI arguments if provided
        self.db_location = os.path.expandvars(os.path.expanduser(db_location)) if db_location else config['db_location']
        self.conda_cache_dir = os.path.expandvars(os.path.expanduser(conda_cache_dir)) if conda_cache_dir else config['conda_cache_dir']
        
        self.force_reinstall = force_reinstall
        self.install_mode = install_mode
        self.conda_only = conda_only
        self.verbose = verbose
        
        # Logging setup
        self._setup_logging()
    
    def _setup_logging(self):
        """
        Configure logging with file and console output
        """
        # Create logs directory if it doesn't exist
        log_dir = Path('./phorager_logs')
        log_dir.mkdir(exist_ok=True)
        
        # Generate log filename with timestamp
        log_filename = log_dir / f"install_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # Configure logging
        logging.basicConfig(
            level=logging.DEBUG if self.verbose else logging.INFO,
            format='%(asctime)s - %(levelname)s: %(message)s',
            handlers=[
                logging.FileHandler(log_filename),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Log the locations being used
        self.logger.info(f"Database Location: {self.db_location}")
        self.logger.info(f"Conda Cache Directory: {self.conda_cache_dir}")
    
    def _validate_nextflow_installation(self):
        """
        Check if Nextflow is installed and accessible
        """
        try:
            subprocess.run(['nextflow', '-version'], 
                           stdout=subprocess.PIPE, 
                           stderr=subprocess.PIPE, 
                           check=True)
        except (subprocess.CalledProcessError, FileNotFoundError):
            self.logger.error(f"{TerminalColors.RED}Nextflow is not installed or not in PATH.{TerminalColors.RESET}")
            sys.exit(1)
    
    def _check_disk_space(self, locations, min_space_gb=100):
        """
        Check available disk space for multiple locations
        """
        for location in locations:
            try:
                # Get parent directory that exists if location doesn't exist
                check_path = location
                while not os.path.exists(check_path):
                    check_path = os.path.dirname(check_path)
                    if check_path == '':  # Root directory check
                        check_path = '/'
                        break
                
                total, used, free = shutil.disk_usage(check_path)
                free_gb = free / (1024**3)
                
                if free_gb < min_space_gb:
                    self.logger.error(
                        f"{TerminalColors.RED}Insufficient disk space for {location}. "
                        f"Required: {min_space_gb}G, Available: {free_gb:.2f}G{TerminalColors.RESET}"
                    )
                    sys.exit(1)
                else:
                    self.logger.info(f"Sufficient disk space available for {location}: {free_gb:.2f}G")
            except Exception as e:
                self.logger.warning(f"Could not check disk space for {location}: {e}")
    
    def _prepare_directories(self):
        """
        Create database and conda cache directories if they don't exist
        """
        for location in [self.db_location, self.conda_cache_dir]:
            os.makedirs(location, exist_ok=True)
            self.logger.info(f"Ensuring directory exists: {location}")


    def _run_nextflow_install(self):
        """
        Run Nextflow installation workflow with comprehensive logging
        """
        # Installation command
        install_cmd = [
            'nextflow', 
            'run', 
            'main.nf', 
            '-profile', 'conda',
            '--workflow', 'install',
            f'--global_db_location', self.db_location,
            f'--conda_cache_dir', self.conda_cache_dir,
            f'--install_mode', self.install_mode.lower(),
            f'--conda_only', str(self.conda_only)
        ]
        
        # Add force flag if specified
        if self.force_reinstall:
            install_cmd.append('--force')
        
        try:
            # Run installation with comprehensive output handling
            self.logger.info(f"{TerminalColors.YELLOW}Starting database installation...{TerminalColors.RESET}")
            
            # Use subprocess.Popen for real-time output capture
            install_process = subprocess.Popen(
                install_cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT, 
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Capture all output
            output_lines = []
            while True:
                output = install_process.stdout.readline()
                if output == '' and install_process.poll() is not None:
                    break
                if output:
                    output = output.strip()
                    output_lines.append(output)
                    
                    # Log all output
                    self.logger.info(output)
                    
                    # Print to console if verbose
                    if self.verbose:
                        print(output)
            
            # Check process return code
            if install_process.returncode != 0:
                # Log full output if installation failed
                for line in output_lines:
                    self.logger.error(f"Installation Error: {line}")
                raise subprocess.CalledProcessError(
                    install_process.returncode, 
                    install_cmd, 
                    '\n'.join(output_lines)
                )
            
            # Log and print installation success
            success_message = f"{TerminalColors.GREEN}phorager installation successful!{TerminalColors.RESET}"
            self.logger.info(success_message)
            print(success_message)
            
            # Clean and remove work directory
            clean_cmd = ['nextflow', 'clean', '-f']
            try:
                # Remove Nextflow cache
                subprocess.run(clean_cmd, check=True, capture_output=not self.verbose, text=True)
                
                # Remove work directory
                work_dir = os.path.join(self._resolve_project_dir(), 'work')
                if os.path.exists(work_dir):
                    shutil.rmtree(work_dir)
                    
                    # Log and print work directory removal
                    rm_message = f"{TerminalColors.GREEN}Nextflow work directory removed successfully!{TerminalColors.RESET}"
                    self.logger.info(rm_message)
                    print(rm_message)
                
            except Exception as clean_error:
                # Log cleanup error
                error_message = f"{TerminalColors.YELLOW}Failed to clean directories: {clean_error}{TerminalColors.RESET}"
                self.logger.warning(error_message)
                print(error_message)
        
        except subprocess.CalledProcessError as e:
            error_message = f"{TerminalColors.RED}Installation failed: {e}{TerminalColors.RESET}"
            self.logger.error(error_message)
            print(error_message)
            sys.exit(1)

    def _resolve_project_dir(self):
        """
        Resolve the project directory based on the current script location
        """
        return os.path.dirname(os.path.abspath(sys.argv[0]))

    def run(self):
            """
            Main installation workflow
            """
            # Validate Nextflow installation
            self._validate_nextflow_installation()

            # Prepare directories
            self._prepare_directories()
            
            # Check disk space for both locations
            self._check_disk_space([self.db_location, self.conda_cache_dir])
            
            # Run installation
            self._run_nextflow_install()

## Bacterial genome subworkflow
class BacterialWorkflowValidator:
    @staticmethod
    def validate_threads(threads):
        """
        Validate thread count:
        - Must be a positive integer
        - Cannot exceed available system cores
        """
        try:
            threads = int(threads)
            max_cores = multiprocessing.cpu_count()
            
            if threads < 1:
                raise ValueError("Thread count must be a positive integer")
            
            if threads > max_cores:
                print(f"Warning: Requested threads ({threads}) exceeds available cores ({max_cores}). Using {max_cores} cores.")
                return max_cores
            
            return threads
        except ValueError:
            raise ValueError(f"Invalid thread count: {threads}. Must be a positive integer.")

    @staticmethod
    def validate_database_location(db_location):
        """
        Validate database location:
        - Must exist
        - Must be a directory
        """
        if not os.path.exists(db_location):
            raise ValueError(f"Database location does not exist: {db_location}")
        
        if not os.path.isdir(db_location):
            raise ValueError(f"Database location is not a directory: {db_location}")
        
        return db_location

    @staticmethod
    def validate_percentage_threshold(value, name):
        """
        Validate percentage thresholds:
        - Must be between 0 and 100
        """
        try:
            float_value = float(value)
            if float_value < 0 or float_value > 100:
                raise ValueError(f"{name} must be between 0 and 100")
            return float_value
        except ValueError:
            raise ValueError(f"Invalid {name}: {value}. Must be a number between 0 and 100.")

    @staticmethod
    def validate_ani_threshold(value):
        """
        Validate ANI threshold:
        - Must be between 0 and 1
        """
        try:
            float_value = float(value)
            if float_value < 0 or float_value > 1:
                raise ValueError("ANI threshold must be between 0 and 1")
            return float_value
        except ValueError:
            raise ValueError(f"Invalid ANI threshold: {value}. Must be a number between 0 and 1.")

    @staticmethod
    def validate_genome(genome_path):
        """
        Validate genome input:
        - Must be a file with .fa, .fasta, or .fna extension
        - Or a directory containing files with .fa, .fasta, or .fna extensions
        """
        if not os.path.exists(genome_path):
            raise ValueError(f"Genome path does not exist: {genome_path}")
        
        if os.path.isfile(genome_path):
            # Check file extension
            if not any(genome_path.endswith(ext) for ext in ['.fa', '.fasta', '.fna']):
                raise ValueError(f"Invalid genome file. Must end with .fa, .fasta, or .fna: {genome_path}")
            return genome_path
        
        if os.path.isdir(genome_path):
            # Check directory contains valid genome files
            genome_files = [
                f for f in os.listdir(genome_path) 
                if any(f.endswith(ext) for ext in ['.fa', '.fasta', '.fna'])
            ]
            
            if not genome_files:
                raise ValueError(f"No genome files found in directory. Files must end with .fa, .fasta, or .fna: {genome_path}")
            
            return genome_path
        
        raise ValueError(f"Invalid genome path: {genome_path}")
    
    @staticmethod
    def validate_directory(directory_path):
        """
        Validate directory:
        - Must exist
        - Must be a directory
        """
        if not os.path.exists(directory_path):
            raise ValueError(f"Directory does not exist: {directory_path}")
        
        if not os.path.isdir(directory_path):
            raise ValueError(f"Path is not a directory: {directory_path}")
        
        return directory_path
    
    @staticmethod
    def validate_output_directory(outdir):
        """
        Validate output directory:
        - If directory doesn't exist, create it
        - If it exists, print a warning
        """
        outdir = os.path.abspath(os.path.expanduser(outdir))
        
        if not os.path.exists(outdir):
            try:
                os.makedirs(outdir)
                print(f"Created output directory: {outdir}")
            except Exception as e:
                raise ValueError(f"Could not create output directory: {outdir}. Error: {e}")
        else:
            print(f"Warning: Using existing output directory: {outdir}")
        
        return outdir

class PhoragerBacterial:
    def __init__(self, 
             genome=None, 
             db_location=None, 
             conda_cache_dir=None,
             outdir=None,
             threads=None, 
             completeness_threshold=None, 
             contamination_threshold=None, 
             drep_ani_threshold=None,
             verbose=False,
             force=False):
    
        # Parse configuration
        config = NextflowConfigParser.parse_config()
        
        # Validate and set genome
        self.genome = BacterialWorkflowValidator.validate_genome(
            genome if genome is not None else config['genome']
        )
        
        # Validate and set database location
        self.db_location = BacterialWorkflowValidator.validate_database_location(
            db_location if db_location is not None else config['db_location']
        )
        
        # Validate and set conda cache directory
        self.conda_cache_dir = BacterialWorkflowValidator.validate_directory(
            conda_cache_dir if conda_cache_dir is not None else config['conda_cache_dir']
        )

        # Validate and set output directory
        self.outdir = BacterialWorkflowValidator.validate_output_directory(
            outdir if outdir is not None else config['outdir']
        )

        # Validate and set threads
        try:
            self.threads = BacterialWorkflowValidator.validate_threads(
                threads if threads is not None else config.get('threads', multiprocessing.cpu_count())
            )
        except KeyError:
            self.threads = BacterialWorkflowValidator.validate_threads(multiprocessing.cpu_count())
                
        # Validate and set completeness threshold
        self.completeness_threshold = BacterialWorkflowValidator.validate_percentage_threshold(
            completeness_threshold if completeness_threshold is not None else config['completeness_threshold'],
            'Completeness threshold'
        )
        
        # Validate and set contamination threshold
        self.contamination_threshold = BacterialWorkflowValidator.validate_percentage_threshold(
            contamination_threshold if contamination_threshold is not None else config['contamination_threshold'],
            'Contamination threshold'
        )
        
        # Validate and set dRep ANI threshold
        self.drep_ani_threshold = BacterialWorkflowValidator.validate_ani_threshold(
            drep_ani_threshold if drep_ani_threshold is not None else config['drep_ani_threshold']
        )
        
        # Set verbose and force flags
        self.verbose = verbose
        self.force = force
        
        # Setup logging
        self._setup_logging()

    def _setup_logging(self):
        """
        Configure logging with file and console output
        """
        # Create logs directory if it doesn't exist
        log_dir = Path('./phorager_logs')
        log_dir.mkdir(exist_ok=True)
        
        # Generate log filename with timestamp
        log_filename = log_dir / f"bacterial_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # Configure logging
        logging.basicConfig(
            level=logging.DEBUG if self.verbose else logging.INFO,
            format='%(asctime)s - %(levelname)s: %(message)s',
            handlers=[
                logging.FileHandler(log_filename),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Log the parameters being used
        self.logger.info(f"Genome: {self.genome}")
        self.logger.info(f"Database Location: {self.db_location}")
        self.logger.info(f"Threads: {self.threads}")
        self.logger.info(f"Completeness Threshold: {self.completeness_threshold}")
        self.logger.info(f"Contamination Threshold: {self.contamination_threshold}")
        self.logger.info(f"dRep ANI Threshold: {self.drep_ani_threshold}")
    
    def _run_nextflow_bacterial(self):
        """
        Run Nextflow bacterial workflow with comprehensive output handling
        """
        # Construct Nextflow command with validated parameters
        cmd = [
            'nextflow', 
            'run', 
            'main.nf', 
            '-profile', 'conda',
            '--workflow', 'bacterial',
            f'--genome', self.genome,
            f'--global_db_location', self.db_location,
            f'--outdir', self.outdir,
            f'--conda_cache_dir', self.conda_cache_dir,
            f'--threads', str(self.threads),
            f'--completeness_threshold', str(self.completeness_threshold),
            f'--contamination_threshold', str(self.contamination_threshold),
            f'--drep_ani_threshold', str(self.drep_ani_threshold)
        ]
        
        try:
            # Run installation with real-time output handling
            self.logger.info(f"Starting bacterial workflow...")
            
            # Use subprocess.Popen for real-time output
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT, 
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # Capture and log output in real-time
            output_lines = []
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    output = output.strip()
                    output_lines.append(output)
                    
                    # Log all output
                    self.logger.info(output)
                    
                    # Print to console if verbose
                    if self.verbose:
                        print(output)
            
            # Check return code
            if process.returncode != 0:
                raise subprocess.CalledProcessError(
                    process.returncode, 
                    cmd, 
                    '\n'.join(output_lines)
                )
            
            # Run cache and work directory cleanup
            try:
                # Nextflow clean command
                clean_cmd = ['nextflow', 'clean', '-f']
                subprocess.run(clean_cmd, check=True, capture_output=not self.verbose, text=True)
                
                # Remove work directory
                work_dir = os.path.join(self._resolve_project_dir(), 'work')
                if os.path.exists(work_dir):
                    shutil.rmtree(work_dir)
                    
                    # Log and print work directory removal
                    rm_message = f"Nextflow work directory removed successfully!"
                    self.logger.info(rm_message)
                    print(rm_message)
                
            except Exception as clean_error:
                # Log cleanup error
                error_message = f"Failed to clean directories: {clean_error}"
                self.logger.warning(error_message)
                print(error_message)
            
            # Log and print success message
            success_message = f"Bacterial workflow completed successfully!"
            self.logger.info(success_message)
            print(success_message)
        
        except subprocess.CalledProcessError as e:
            error_message = f"Bacterial workflow failed: {e}"
            self.logger.error(error_message)
            print(error_message)
            sys.exit(1)

    def _resolve_project_dir(self):
        """
        Resolve the project directory based on the current script location
        """
        return os.path.dirname(os.path.abspath(sys.argv[0]))
 
    def run(self):
        """
        Main workflow execution
        """
        self._run_nextflow_bacterial()

## Prophage detection subworkflow
class ProphageWorkflowValidator:
    @staticmethod
    def validate_genome(genome_path):
        """
        Validate genome input
        - Must be a file or directory with .fa, .fasta, or .fna files
        """
        if not os.path.exists(genome_path):
            raise ValueError(f"Genome path does not exist: {genome_path}")
        
        if os.path.isfile(genome_path):
            if not any(genome_path.endswith(ext) for ext in ['.fa', '.fasta', '.fna']):
                raise ValueError(f"Invalid genome file. Must end with .fa, .fasta, or .fna: {genome_path}")
            return genome_path
        
        if os.path.isdir(genome_path):
            genome_files = [
                f for f in os.listdir(genome_path) 
                if any(f.endswith(ext) for ext in ['.fa', '.fasta', '.fna'])
            ]
            
            if not genome_files:
                raise ValueError(f"No genome files found in directory. Files must end with .fa, .fasta, or .fna: {genome_path}")
            
            return genome_path
        
        raise ValueError(f"Invalid genome path: {genome_path}")

    @staticmethod
    def validate_database_location(db_location):
        """
        Validate database location
        """
        if not os.path.exists(db_location):
            raise ValueError(f"Database location does not exist: {db_location}")
        
        if not os.path.isdir(db_location):
            raise ValueError(f"Database location is not a directory: {db_location}")
        
        return db_location

    @staticmethod
    def validate_threads(threads):
        """
        Validate thread count
        """
        try:
            threads = int(threads)
            max_cores = multiprocessing.cpu_count()
            
            if threads < 1:
                raise ValueError("Thread count must be a positive integer")
            
            if threads > max_cores:
                print(f"Warning: Requested threads ({threads}) exceeds available cores ({max_cores}). Using {max_cores} cores.")
                return max_cores
            
            return threads
        except ValueError:
            raise ValueError(f"Invalid thread count: {threads}. Must be a positive integer.")

class PhoragerProphage:
    def __init__(self, 
                 genome=None,
                 db_location=None,
                 conda_cache_dir=None,
                 outdir=None,
                 threads=None,
                 use_dereplicated_genomes=None,
                 skip_genomad=None,
                 skip_vibrant=None,
                 verbose=False,
                 force=False):
        
        # Parse configuration
        config = NextflowConfigParser.parse_config()
        
        # Validate and set genome
        self.genome = ProphageWorkflowValidator.validate_genome(
            genome if genome is not None else config['genome']
        )
        
        # Validate and set database location
        self.db_location = ProphageWorkflowValidator.validate_database_location(
            db_location if db_location is not None else config['db_location']
        )
        
        # Validate and set conda cache directory
        self.conda_cache_dir = ProphageWorkflowValidator.validate_database_location(
            conda_cache_dir if conda_cache_dir is not None else config['conda_cache_dir']
        )
        
        # Validate and set output directory
        self.outdir = ProphageWorkflowValidator.validate_database_location(
            outdir if outdir is not None else config['outdir']
        )
        
        # Validate and set threads
        self.threads = ProphageWorkflowValidator.validate_threads(
            threads if threads is not None else config['threads']
        )
        
        # Set boolean flags
        self.use_dereplicated_genomes = True if use_dereplicated_genomes else config['use_dereplicated_genomes']
        self.run_genomad = not (skip_genomad if skip_genomad is not None else False)
        self.run_vibrant = not (skip_vibrant if skip_vibrant is not None else False)
        
        self.verbose = verbose
        self.force = force
        
        # Setup logging
        self._setup_logging()
    
    def _setup_logging(self):
        """
        Configure logging with file and console output
        """
        log_dir = Path('./phorager_logs')
        log_dir.mkdir(exist_ok=True)
        
        log_filename = log_dir / f"prophage_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.DEBUG if self.verbose else logging.INFO,
            format='%(asctime)s - %(levelname)s: %(message)s',
            handlers=[
                logging.FileHandler(log_filename),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Log parameters
        self.logger.info(f"Genome: {self.genome}")
        self.logger.info(f"Database Location: {self.db_location}")
        self.logger.info(f"Threads: {self.threads}")
        self.logger.info(f"Use Dereplicated Genomes: {self.use_dereplicated_genomes}")
        self.logger.info(f"Run geNomad: {self.run_genomad}")
        self.logger.info(f"Run VIBRANT: {self.run_vibrant}")
    
    def _run_nextflow_prophage(self):
        """
        Run Nextflow prophage workflow
        """
        cmd = [
            'nextflow', 
            'run', 
            'main.nf', 
            '-profile', 'conda',
            '--workflow', 'prophage',
            f'--genome', self.genome,
            f'--global_db_location', self.db_location,
            f'--conda_cache_dir', self.conda_cache_dir,
            f'--outdir', self.outdir,
            f'--threads', str(self.threads),
            f'--use_dereplicated_genomes', str(self.use_dereplicated_genomes),
            f'--run_genomad', str(self.run_genomad),
            f'--run_vibrant', str(self.run_vibrant)
        ]
        
        try:
            self.logger.info("Starting prophage workflow...")
            
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT, 
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            output_lines = []
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    output = output.strip()
                    output_lines.append(output)
                    
                    self.logger.info(output)
                    
                    if self.verbose:
                        print(output)
            
            if process.returncode != 0:
                raise subprocess.CalledProcessError(
                    process.returncode, 
                    cmd, 
                    '\n'.join(output_lines)
                )
            
            # Cleanup
            clean_cmd = ['nextflow', 'clean', '-f']
            subprocess.run(clean_cmd, check=True, capture_output=not self.verbose, text=True)
            
            work_dir = os.path.join(self._resolve_project_dir(), 'work')
            if os.path.exists(work_dir):
                shutil.rmtree(work_dir)
            
            success_message = "Prophage workflow completed successfully!"
            self.logger.info(success_message)
            print(success_message)
        
        except subprocess.CalledProcessError as e:
            error_message = f"Prophage workflow failed: {e}"
            self.logger.error(error_message)
            print(error_message)
            sys.exit(1)
    
    def _resolve_project_dir(self):
        """
        Resolve the project directory
        """
        return os.path.dirname(os.path.abspath(sys.argv[0]))
    
    def run(self):
        """
        Main workflow execution
        """
        self._run_nextflow_prophage()

## Prophage annotation subworkflow
class AnnotationWorkflowValidator:
    @staticmethod
    def validate_prophage_fasta(prophage_fasta):
        """
        Validate prophage FASTA input
        - Must be an existing file
        - Must have .fasta, .fa, or .fna extension
        """
        if not os.path.exists(prophage_fasta):
            raise ValueError(f"Prophage FASTA file does not exist: {prophage_fasta}")
        
        if not any(prophage_fasta.endswith(ext) for ext in ['.fasta', '.fa', '.fna']):
            raise ValueError(f"Invalid file type. Must be .fasta, .fa, or .fna: {prophage_fasta}")
        
        return prophage_fasta

    @staticmethod
    def validate_database_location(db_location):
        """
        Validate database location
        """
        if not os.path.exists(db_location):
            raise ValueError(f"Database location does not exist: {db_location}")
        
        if not os.path.isdir(db_location):
            raise ValueError(f"Database location is not a directory: {db_location}")
        
        return db_location

    @staticmethod
    def validate_threads(threads):
        """
        Validate thread count
        """
        try:
            threads = int(threads)
            max_cores = multiprocessing.cpu_count()
            
            if threads < 1:
                raise ValueError("Thread count must be a positive integer")
            
            if threads > max_cores:
                print(f"Warning: Requested threads ({threads}) exceeds available cores ({max_cores}). Using {max_cores} cores.")
                return max_cores
            
            return threads
        except ValueError:
            raise ValueError(f"Invalid thread count: {threads}. Must be a positive integer.")

    @staticmethod
    def validate_percentage(value, name):
        """
        Validate percentage values
        """
        try:
            float_value = float(value)
            if float_value < 0 or float_value > 100:
                raise ValueError(f"{name} must be between 0 and 100")
            return float_value
        except ValueError:
            raise ValueError(f"Invalid {name}: {value}. Must be a number between 0 and 100.")

    @staticmethod
    def validate_integer(value, name):
        """
        Validate integer values
        """
        try:
            int_value = int(value)
            if int_value < 0:
                raise ValueError(f"{name} must be a non-negative integer")
            return int_value
        except ValueError:
            raise ValueError(f"Invalid {name}: {value}. Must be a non-negative integer.")

    @staticmethod
    def validate_clustering_params(ani, coverage):
        """
        Validate clustering parameters
        """
        try:
            ani_value = float(ani)
            coverage_value = float(coverage)
            
            if ani_value < 0 or ani_value > 100:
                raise ValueError("ANI must be between 0 and 100")
            
            if coverage_value < 0 or coverage_value > 100:
                raise ValueError("Coverage must be between 0 and 100")
            
            return ani_value, coverage_value
        except ValueError as e:
            raise ValueError(f"Invalid clustering parameters: {e}")

    @staticmethod
    def validate_checkv_quality_levels(quality_levels):
        """
        Validate CheckV quality levels
        - Must be a list or tuple
        - Each level must be a string
        - Must contain at least one valid quality level
        """
        # Valid quality levels
        valid_levels = ['Low-quality', 'Medium-quality', 'High-quality', 'Complete']
        
        # Check if input is a list or tuple
        if not isinstance(quality_levels, (list, tuple)):
            raise ValueError("CheckV quality levels must be a list or tuple")
        
        # Ensure list is not empty
        if not quality_levels:
            raise ValueError("CheckV quality levels list cannot be empty")
        
        # Validate each level
        for level in quality_levels:
            if not isinstance(level, str):
                raise ValueError(f"Quality level must be a string, got {type(level)}")
            
            if level not in valid_levels:
                raise ValueError(f"Invalid quality level: {level}. Must be one of {valid_levels}")
        
        return quality_levels

class PhoragerAnnotation:
    def __init__(self, 
                prophage_fasta=None,
                db_location=None,
                conda_cache_dir=None,
                outdir=None,
                threads=None,
                min_prophage_length=None,
                checkv_quality_levels=None,
                skip_detailed_annotation=None,
                annotation_filter_mode=None,
                pharokka_structural_perc=None,
                pharokka_structural_total=None,
                phold_structural_perc=None,
                phold_structural_total=None,
                clustering_min_ani=None,
                clustering_min_coverage=None,
                verbose=False,
                force=False):
        
        # Parse configuration
        config = NextflowConfigParser.parse_config()
        
        # Validate and set prophage FASTA
        self.prophage_fasta = (
            AnnotationWorkflowValidator.validate_prophage_fasta(prophage_fasta) 
            if prophage_fasta 
            else None
        )
        
        # Validate and set database location
        self.db_location = AnnotationWorkflowValidator.validate_database_location(
            db_location if db_location is not None else config['db_location']
        )
        
        # Validate and set conda cache directory
        self.conda_cache_dir = AnnotationWorkflowValidator.validate_database_location(
            conda_cache_dir if conda_cache_dir else config['conda_cache_dir']
        )
        
        # Validate and set output directory
        self.outdir = AnnotationWorkflowValidator.validate_database_location(
            outdir if outdir else config['outdir']
        )
        
        # Validate and set threads
        self.threads = AnnotationWorkflowValidator.validate_threads(
            threads if threads is not None else config['threads']
        )
        
        # Validate and set prophage length
        self.min_prophage_length = AnnotationWorkflowValidator.validate_integer(
            min_prophage_length if min_prophage_length is not None else config['min_prophage_length'],
            'Minimum prophage length'
        )
        
        # Set skip detailed annotation
        self.skip_detailed_annotation = (
            skip_detailed_annotation 
            if skip_detailed_annotation is not None 
            else config['skip_detailed_annotation']
        )
        
        # Validate and set clustering parameters
        self.clustering_min_ani, self.clustering_min_coverage = AnnotationWorkflowValidator.validate_clustering_params(
            clustering_min_ani if clustering_min_ani is not None else config['clustering_min_ani'],
            clustering_min_coverage if clustering_min_coverage is not None else config['clustering_min_coverage']
        )
        
        # Validate and set annotation filter parameters
        self.annotation_filter_mode = (
            annotation_filter_mode 
            if annotation_filter_mode is not None 
            else config['annotation_filter_mode']
        )
        
        self.pharokka_structural_perc = AnnotationWorkflowValidator.validate_percentage(
            pharokka_structural_perc if pharokka_structural_perc is not None else config['pharokka_structural_perc'],
            'Pharokka structural percentage'
        )
        
        self.pharokka_structural_total = AnnotationWorkflowValidator.validate_integer(
            pharokka_structural_total if pharokka_structural_total is not None else config['pharokka_structural_total'],
            'Pharokka structural total'
        )
        
        self.phold_structural_perc = AnnotationWorkflowValidator.validate_percentage(
            phold_structural_perc if phold_structural_perc is not None else config['phold_structural_perc'],
            'PHOLD structural percentage'
        )
        
        self.phold_structural_total = AnnotationWorkflowValidator.validate_integer(
            phold_structural_total if phold_structural_total is not None else config['phold_structural_total'],
            'PHOLD structural total'
        )
        
        # Validate CheckV quality levels
        self.checkv_quality_levels = (
            AnnotationWorkflowValidator.validate_checkv_quality_levels(
                checkv_quality_levels 
                if checkv_quality_levels is not None 
                else config['checkv_quality_levels']
            )
        )

        # Set verbose and force flags
        self.verbose = verbose
        self.force = force
        
        # Setup logging
        self._setup_logging()
    
    def _setup_logging(self):
        """
        Configure logging with file and console output
        """
        log_dir = Path('./phorager_logs')
        log_dir.mkdir(exist_ok=True)
        
        log_filename = log_dir / f"annotation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.DEBUG if self.verbose else logging.INFO,
            format='%(asctime)s - %(levelname)s: %(message)s',
            handlers=[
                logging.FileHandler(log_filename),
                logging.StreamHandler(sys.stdout)
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Log parameters
        self.logger.info(f"Prophage FASTA: {self.prophage_fasta}")
        self.logger.info(f"Database Location: {self.db_location}")
        self.logger.info(f"Threads: {self.threads}")
        self.logger.info(f"Minimum Prophage Length: {self.min_prophage_length}")
        self.logger.info(f"Skip Detailed Annotation: {self.skip_detailed_annotation}")
    
    def _run_nextflow_annotation(self):
        """
        Run Nextflow annotation workflow
        """
        cmd = [
            'nextflow', 
            'run', 
            'main.nf', 
            '-profile', 'conda',
            '--workflow', 'annotation',
        ]
        
        # Add optional parameters
        if self.prophage_fasta:
            cmd.extend([f'--prophage_fasta', self.prophage_fasta])
        
        cmd.extend([
            f'--global_db_location', self.db_location,
            f'--conda_cache_dir', self.conda_cache_dir,
            f'--outdir', self.outdir,
            f'--threads', str(self.threads),
            f'--min_prophage_length', str(self.min_prophage_length),
            f'--skip_detailed_annotation', str(self.skip_detailed_annotation),
            f'--annotation_filter_mode', self.annotation_filter_mode,
            f'--pharokka_structural_perc', str(self.pharokka_structural_perc),
            f'--pharokka_structural_total', str(self.pharokka_structural_total),
            f'--phold_structural_perc', str(self.phold_structural_perc),
            f'--phold_structural_total', str(self.phold_structural_total),
            f'--clustering_min_ani', str(self.clustering_min_ani),
            f'--clustering_min_coverage', str(self.clustering_min_coverage)
        ])
        
        try:
            self.logger.info("Starting annotation workflow...")
            
            process = subprocess.Popen(
                cmd, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.STDOUT, 
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            output_lines = []
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    output = output.strip()
                    output_lines.append(output)
                    
                    self.logger.info(output)
                    
                    if self.verbose:
                        print(output)
            
            if process.returncode != 0:
                raise subprocess.CalledProcessError(
                    process.returncode, 
                    cmd, 
                    '\n'.join(output_lines)
                )
            
            # Cleanup
            clean_cmd = ['nextflow', 'clean', '-f']
            subprocess.run(clean_cmd, check=True, capture_output=not self.verbose, text=True)
            
            work_dir = os.path.join(self._resolve_project_dir(), 'work')
            if os.path.exists(work_dir):
                shutil.rmtree(work_dir)
            
            success_message = "Annotation workflow completed successfully!"
            self.logger.info(success_message)
            print(success_message)
        
        except subprocess.CalledProcessError as e:
            error_message = f"Annotation workflow failed: {e}"
            self.logger.error(error_message)
            print(error_message)
            sys.exit(1)
    
    def _resolve_project_dir(self):
        """
        Resolve the project directory
        """
        return os.path.dirname(os.path.abspath(sys.argv[0]))
    
    def run(self):
        """
        Main workflow execution
        """
        if self.prophage_fasta is None:
            self.logger.error("No prophage FASTA file provided. This is required when running the annotation workflow directly.")
            print("Error: No prophage FASTA file provided. This is required when running the annotation workflow directly.")
            sys.exit(1)
        
        self._run_nextflow_annotation()

## MAIN
def main():    
    # Create the top-level parser
    parser = argparse.ArgumentParser(description='Phorager: Nextflow Pipeline Database Installer and Workflow Runner')
    
    # Create subparsers
    subparsers = parser.add_subparsers(dest='command', help='Subcommands')
    
    # Install subcommand
    install_parser = subparsers.add_parser('install', help='Install databases')
    install_parser.add_argument('--mode',choices=['all', 'genome', 'prophage', 'annotation'], default='all', help='Installation mode: all components or specific subsets')
    install_parser.add_argument('--db-location', help='Custom database installation location')
    install_parser.add_argument('--conda-cache', help='Custom conda cache directory')
    install_parser.add_argument('--force', action='store_true', help='Force reinstall all databases')
    install_parser.add_argument('--conda-only', action='store_true', help='Install only Conda environments, skip database downloads')
    install_parser.add_argument('--verbose', action='store_true', help='Enable verbose logging')
    
    # Bacterial subcommand
    bacterial_parser = subparsers.add_parser('bacterial', help='Run bacterial workflow')
    bacterial_parser.add_argument('--genome', help='Path to genome file or directory')
    bacterial_parser.add_argument('--db-location', help='Database location')
    bacterial_parser.add_argument('--outdir', help='Output directory')
    bacterial_parser.add_argument('--conda-cache', help='Conda cache directory location')
    bacterial_parser.add_argument('--threads', type=int, help='Number of threads')
    bacterial_parser.add_argument('--completeness-threshold', type=float, help='Completeness threshold')
    bacterial_parser.add_argument('--contamination-threshold', type=float, help='Contamination threshold')
    bacterial_parser.add_argument('--drep-ani-threshold', type=float, help='dRep ANI threshold')
    bacterial_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    bacterial_parser.add_argument('--force', action='store_true', help='Force overwrite')
    
    # Prophage detection parser
    prophage_parser = subparsers.add_parser('prophage', help='Run prophage workflow')
    prophage_parser.add_argument('--genome', help='Path to genome file or directory')
    prophage_parser.add_argument('--db-location', help='Database location')
    prophage_parser.add_argument('--conda-cache', help='Conda cache directory location')
    prophage_parser.add_argument('--outdir', help='Output directory')
    prophage_parser.add_argument('--threads', type=int, help='Number of threads')
    prophage_parser.add_argument('--use-dereplicated-genomes', action='store_true', help='Use dereplicated genomes')
    prophage_parser.add_argument('--skip-genomad', action='store_true', help='Skip geNomad analysis')
    prophage_parser.add_argument('--skip-vibrant', action='store_true', help='Skip VIBRANT analysis')
    prophage_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    prophage_parser.add_argument('--force', action='store_true', help='Force overwrite')

    # Prophage annotation parser
    annotation_parser = subparsers.add_parser('annotation', help='Run annotation workflow')
    annotation_parser.add_argument('--prophage-fasta', help='Path to prophage FASTA file')
    annotation_parser.add_argument('--db-location', help='Database location')
    annotation_parser.add_argument('--conda-cache', help='Conda cache directory location')
    annotation_parser.add_argument('--outdir', help='Output directory')
    annotation_parser.add_argument('--threads', type=int, help='Number of threads')
    annotation_parser.add_argument('--min-prophage-length', type=int, help='Minimum prophage length')
    annotation_parser.add_argument('--skip-detailed-annotation', action='store_true', help='Skip detailed annotation')
    annotation_parser.add_argument('--annotation-filter-mode', help='Annotation filter mode')
    annotation_parser.add_argument('--pharokka-structural-perc', type=float, help='Pharokka structural percentage')
    annotation_parser.add_argument('--pharokka-structural-total', type=int, help='Pharokka structural total')
    annotation_parser.add_argument('--phold-structural-perc', type=float, help='PHOLD structural percentage')
    annotation_parser.add_argument('--phold-structural-total', type=int, help='PHOLD structural total')
    annotation_parser.add_argument('--clustering-min-ani', type=float, help='Minimum ANI for clustering')
    annotation_parser.add_argument('--clustering-min-coverage', type=float, help='Minimum coverage for clustering')
    annotation_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    annotation_parser.add_argument('--force', action='store_true', help='Force overwrite')

    # Add end-to-end subparser with ALL parameters from individual workflows
    end_to_end_parser = subparsers.add_parser('run', help='Run entire prophage detection pipeline')
    # Bacterial workflow parameters
    end_to_end_parser.add_argument('--genome', help='Path to input genomes', required=True)
    end_to_end_parser.add_argument('--db-location', help='Database location')
    end_to_end_parser.add_argument('--conda-cache', help='Conda cache directory')
    end_to_end_parser.add_argument('--threads', type=int, help='Number of threads')
    end_to_end_parser.add_argument('--completeness-threshold', type=float, help='Minimum genome completeness')
    end_to_end_parser.add_argument('--contamination-threshold', type=float, help='Maximum genome contamination')
    end_to_end_parser.add_argument('--drep-ani-threshold', type=float, help='ANI threshold for dereplication')
    # Prophage workflow parameters
    end_to_end_parser.add_argument('--skip-genomad', action='store_true', help='Skip geNomad analysis')
    end_to_end_parser.add_argument('--skip-vibrant', action='store_true', help='Skip VIBRANT analysis')
    # Annotation workflow parameters
    end_to_end_parser.add_argument('--min-prophage-length', type=int, help='Minimum prophage length')
    end_to_end_parser.add_argument('--skip-detailed-annotation', action='store_true', help='Skip detailed annotation')
    end_to_end_parser.add_argument('--annotation-filter-mode', help='Annotation filter mode')
    end_to_end_parser.add_argument('--pharokka-structural-perc', type=float, help='Pharokka structural percentage')
    end_to_end_parser.add_argument('--pharokka-structural-total', type=int, help='Pharokka structural total')
    end_to_end_parser.add_argument('--phold-structural-perc', type=float, help='PHOLD structural percentage')
    end_to_end_parser.add_argument('--phold-structural-total', type=int, help='PHOLD structural total')
    end_to_end_parser.add_argument('--clustering-min-ani', type=float, help='Minimum ANI for clustering')
    end_to_end_parser.add_argument('--clustering-min-coverage', type=float, help='Minimum coverage for clustering')
    # Common parameters
    end_to_end_parser.add_argument('--outdir', help='Output directory', default='results')
    end_to_end_parser.add_argument('--verbose', action='store_true', help='Enable verbose output')
    end_to_end_parser.add_argument('--force', action='store_true', help='Force overwrite')

    # Parse arguments
    args = parser.parse_args()
    
    # Check if a subcommand was provided
    if args.command is None:
        parser.print_help()
        sys.exit(1)
    
    # Run install workflow
    if args.command == 'install':
        installer = PhoragerInstaller(
            db_location=args.db_location,
            conda_cache_dir=args.conda_cache,
            force_reinstall=args.force,
            install_mode=args.mode,
            conda_only=args.conda_only,
            verbose=args.verbose
        )
        installer.run()
        
    # Run bacterial workflow
    elif args.command == 'bacterial':
        try:
            workflow = PhoragerBacterial(
                genome=args.genome,
                db_location=args.db_location,
                conda_cache_dir=args.conda_cache,
                outdir=args.outdir,
                threads=args.threads,
                completeness_threshold=args.completeness_threshold,
                contamination_threshold=args.contamination_threshold,
                drep_ani_threshold=args.drep_ani_threshold,
                verbose=args.verbose,
                force=args.force
            )
            workflow.run()
        except Exception as e:
            print(f"Error: {e}")
            sys.exit(1)
    
    # Run Propgae workflow
    elif args.command == 'prophage':
        try:
            workflow = PhoragerProphage(
                genome=args.genome,
                db_location=args.db_location,
                conda_cache_dir=args.conda_cache,
                outdir=args.outdir,
                threads=args.threads,
                use_dereplicated_genomes=args.use_dereplicated_genomes,
                skip_genomad=args.skip_genomad,
                skip_vibrant=args.skip_vibrant,
                verbose=args.verbose,
                force=args.force
            )
            workflow.run()
        except Exception as e:
            print(f"Error: {e}")
            sys.exit(1)
    
    # Run annotation workflow
    elif args.command == 'annotation':
        try:
            workflow = PhoragerAnnotation(
                prophage_fasta=args.prophage_fasta,
                db_location=args.db_location,
                conda_cache_dir=args.conda_cache,
                outdir=args.outdir,
                threads=args.threads,
                min_prophage_length=args.min_prophage_length,
                skip_detailed_annotation=args.skip_detailed_annotation,
                annotation_filter_mode=args.annotation_filter_mode,
                pharokka_structural_perc=args.pharokka_structural_perc,
                pharokka_structural_total=args.pharokka_structural_total,
                phold_structural_perc=args.phold_structural_perc,
                phold_structural_total=args.phold_structural_total,
                clustering_min_ani=args.clustering_min_ani,
                clustering_min_coverage=args.clustering_min_coverage,
                verbose=args.verbose,
                force=args.force
            )
            workflow.run()
        except Exception as e:
            print(f"Error: {e}")
            sys.exit(1)

    # End-to-end workflow
    elif args.command == 'run':
        try:
            # Set output directory
            os.makedirs(args.outdir, exist_ok=True)
            
            # 1. Bacterial Workflow
            print("Starting Bacterial Workflow...")
            bacterial_workflow = PhoragerBacterial(
                genome=args.genome,
                db_location=args.db_location,
                conda_cache_dir=args.conda_cache,
                outdir=args.outdir,
                threads=args.threads,
                completeness_threshold=args.completeness_threshold,
                contamination_threshold=args.contamination_threshold,
                drep_ani_threshold=args.drep_ani_threshold,
                verbose=args.verbose,
                force=args.force
            )
            bacterial_workflow.run()
            
            # 2. Prophage Workflow
            print("Starting Prophage Workflow...")
            prophage_workflow = PhoragerProphage(
                genome=f"{args.outdir}/1.Genome_preprocessing/Bact3_dRep/drep_output/dereplicated_genomes",
                use_dereplicated_genomes=True,
                db_location=args.db_location,
                conda_cache_dir=args.conda_cache,
                outdir=args.outdir,
                threads=args.threads,
                skip_genomad=args.skip_genomad,
                skip_vibrant=args.skip_vibrant,
                verbose=args.verbose,
                force=args.force
            )
            prophage_workflow.run()
            
            # 3. Annotation Workflow
            print("Starting Annotation Workflow...")
            annotation_workflow = PhoragerAnnotation(
                prophage_fasta=f"{args.outdir}/2.Prophage_detection/All_prophage_sequences.fasta",
                db_location=args.db_location,
                conda_cache_dir=args.conda_cache,
                outdir=args.outdir,
                threads=args.threads,
                min_prophage_length=args.min_prophage_length,
                skip_detailed_annotation=args.skip_detailed_annotation,
                annotation_filter_mode=args.annotation_filter_mode,
                pharokka_structural_perc=args.pharokka_structural_perc,
                pharokka_structural_total=args.pharokka_structural_total,
                phold_structural_perc=args.phold_structural_perc,
                phold_structural_total=args.phold_structural_total,
                clustering_min_ani=args.clustering_min_ani,
                clustering_min_coverage=args.clustering_min_coverage,
                verbose=args.verbose,
                force=args.force
            )
            annotation_workflow.run()
            
            print("End-to-end workflow completed successfully!")
        
        except Exception as e:
            print(f"Workflow failed: {e}")
            sys.exit(1)

if __name__ == '__main__':
    main()
